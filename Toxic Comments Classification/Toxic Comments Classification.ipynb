{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e2465a",
   "metadata": {},
   "source": [
    "# Поиск \"токсичных\" комментариев.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43067c",
   "metadata": {},
   "source": [
    "# Описание проекта\n",
    "\n",
    "Нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. В наличии есть набор данных с разметкой о токсичности комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c569bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916a6df",
   "metadata": {},
   "source": [
    "##  Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685d306d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv',index_col=[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bacb413",
   "metadata": {},
   "source": [
    "Посмотрим на целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64740a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db72f10",
   "metadata": {},
   "source": [
    "Явно тут дисбаланс классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10fafb",
   "metadata": {},
   "source": [
    "Проверим явные дубликаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f81e7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190f0d4",
   "metadata": {},
   "source": [
    "Удалим лишние символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f281e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z]',' ',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794648",
   "metadata": {},
   "source": [
    "Избавимся от пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a494ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: ' '.join(x.split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e76488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation Why the edits made under my userna...      0\n",
       "1  D aww He matches this background colour I m se...      0\n",
       "2  Hey man I m really not trying to edit war It s...      0\n",
       "3  More I can t make any real suggestions on impr...      0\n",
       "4  You sir are my hero Any chance you remember wh...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b862c",
   "metadata": {},
   "source": [
    "Проведем лемматизацию текста. Используем библиотеку nltk. Чтобы поменялись глаголы мспользуем pos_tag из nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12aee5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sergey.Polushkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sergey.Polushkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "data['text_lemm'] = data['text'].apply(lambda x: ' '.join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e721b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not try to edit war It s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir be my hero Any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation Why the edits made under my userna...      0   \n",
       "1  D aww He matches this background colour I m se...      0   \n",
       "2  Hey man I m really not trying to edit war It s...      0   \n",
       "3  More I can t make any real suggestions on impr...      0   \n",
       "4  You sir are my hero Any chance you remember wh...      0   \n",
       "\n",
       "                                           text_lemm  \n",
       "0  Explanation Why the edits make under my userna...  \n",
       "1  D aww He match this background colour I m seem...  \n",
       "2  Hey man I m really not try to edit war It s ju...  \n",
       "3  More I can t make any real suggestion on impro...  \n",
       "4  You sir be my hero Any chance you remember wha...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971b856",
   "metadata": {},
   "source": [
    "Глаголы поменялись!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2101ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_lemm'] = data['text_lemm'].str.lower()\n",
    "#corpus = data['text_lemm'].values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c8ae3",
   "metadata": {},
   "source": [
    "Получим список стоп-слов для английского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55237647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sergey.Polushkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6054df7",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9330ec5",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовые выборки. Оказывается random_state везде в примерах ставят 42, потому что это ответ на вопрос \"В чём смысл жизни и вообще?\" из Автостопом по галактике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57bcf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text_lemm'], data['toxic'], test_size=0.25, random_state=42,stratify=data['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1f4e3",
   "metadata": {},
   "source": [
    "Получим TF-IDF для корпуса текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) \n",
    "tf_idf_train = count_tf_idf.fit_transform(X_train)\n",
    "tf_idf_test = count_tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0c216",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f812bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg = LogisticRegressionCV(cv=3,scoring='f1', class_weight='balanced' ,solver='liblinear', random_state=42).fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f077ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8687608821822403"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f7897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dict = {}\n",
    "valid_dict['LogisticRegression'] = [reg.score(tf_idf_train, y_train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbba5ec",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09703810",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV 1/3; 1/9] START max_depth=5, n_estimators=50................................\n",
      "[CV 1/3; 1/9] END ..............max_depth=5, n_estimators=50; total time=   4.9s\n",
      "[CV 2/3; 1/9] START max_depth=5, n_estimators=50................................\n",
      "[CV 2/3; 1/9] END ..............max_depth=5, n_estimators=50; total time=   5.4s\n",
      "[CV 3/3; 1/9] START max_depth=5, n_estimators=50................................\n",
      "[CV 3/3; 1/9] END ..............max_depth=5, n_estimators=50; total time=   5.0s\n",
      "[CV 1/3; 2/9] START max_depth=5, n_estimators=70................................\n",
      "[CV 1/3; 2/9] END ..............max_depth=5, n_estimators=70; total time=   6.7s\n",
      "[CV 2/3; 2/9] START max_depth=5, n_estimators=70................................\n",
      "[CV 2/3; 2/9] END ..............max_depth=5, n_estimators=70; total time=   6.8s\n",
      "[CV 3/3; 2/9] START max_depth=5, n_estimators=70................................\n",
      "[CV 3/3; 2/9] END ..............max_depth=5, n_estimators=70; total time=   6.7s\n",
      "[CV 1/3; 3/9] START max_depth=5, n_estimators=100...............................\n",
      "[CV 1/3; 3/9] END .............max_depth=5, n_estimators=100; total time=  10.0s\n",
      "[CV 2/3; 3/9] START max_depth=5, n_estimators=100...............................\n",
      "[CV 2/3; 3/9] END .............max_depth=5, n_estimators=100; total time=   9.5s\n",
      "[CV 3/3; 3/9] START max_depth=5, n_estimators=100...............................\n",
      "[CV 3/3; 3/9] END .............max_depth=5, n_estimators=100; total time=   9.5s\n",
      "[CV 1/3; 4/9] START max_depth=10, n_estimators=50...............................\n",
      "[CV 1/3; 4/9] END .............max_depth=10, n_estimators=50; total time=   9.3s\n",
      "[CV 2/3; 4/9] START max_depth=10, n_estimators=50...............................\n",
      "[CV 2/3; 4/9] END .............max_depth=10, n_estimators=50; total time=   9.3s\n",
      "[CV 3/3; 4/9] START max_depth=10, n_estimators=50...............................\n",
      "[CV 3/3; 4/9] END .............max_depth=10, n_estimators=50; total time=   9.3s\n",
      "[CV 1/3; 5/9] START max_depth=10, n_estimators=70...............................\n",
      "[CV 1/3; 5/9] END .............max_depth=10, n_estimators=70; total time=  12.9s\n",
      "[CV 2/3; 5/9] START max_depth=10, n_estimators=70...............................\n",
      "[CV 2/3; 5/9] END .............max_depth=10, n_estimators=70; total time=  12.9s\n",
      "[CV 3/3; 5/9] START max_depth=10, n_estimators=70...............................\n",
      "[CV 3/3; 5/9] END .............max_depth=10, n_estimators=70; total time=  12.9s\n",
      "[CV 1/3; 6/9] START max_depth=10, n_estimators=100..............................\n",
      "[CV 1/3; 6/9] END ............max_depth=10, n_estimators=100; total time=  18.6s\n",
      "[CV 2/3; 6/9] START max_depth=10, n_estimators=100..............................\n",
      "[CV 2/3; 6/9] END ............max_depth=10, n_estimators=100; total time=  18.5s\n",
      "[CV 3/3; 6/9] START max_depth=10, n_estimators=100..............................\n",
      "[CV 3/3; 6/9] END ............max_depth=10, n_estimators=100; total time=  18.6s\n",
      "[CV 1/3; 7/9] START max_depth=15, n_estimators=50...............................\n",
      "[CV 1/3; 7/9] END .............max_depth=15, n_estimators=50; total time=  14.4s\n",
      "[CV 2/3; 7/9] START max_depth=15, n_estimators=50...............................\n",
      "[CV 2/3; 7/9] END .............max_depth=15, n_estimators=50; total time=  14.7s\n",
      "[CV 3/3; 7/9] START max_depth=15, n_estimators=50...............................\n",
      "[CV 3/3; 7/9] END .............max_depth=15, n_estimators=50; total time=  13.9s\n",
      "[CV 1/3; 8/9] START max_depth=15, n_estimators=70...............................\n",
      "[CV 1/3; 8/9] END .............max_depth=15, n_estimators=70; total time=  19.6s\n",
      "[CV 2/3; 8/9] START max_depth=15, n_estimators=70...............................\n",
      "[CV 2/3; 8/9] END .............max_depth=15, n_estimators=70; total time=  19.5s\n",
      "[CV 3/3; 8/9] START max_depth=15, n_estimators=70...............................\n",
      "[CV 3/3; 8/9] END .............max_depth=15, n_estimators=70; total time=  19.7s\n",
      "[CV 1/3; 9/9] START max_depth=15, n_estimators=100..............................\n",
      "[CV 1/3; 9/9] END ............max_depth=15, n_estimators=100; total time=  28.1s\n",
      "[CV 2/3; 9/9] START max_depth=15, n_estimators=100..............................\n",
      "[CV 2/3; 9/9] END ............max_depth=15, n_estimators=100; total time=  28.1s\n",
      "[CV 3/3; 9/9] START max_depth=15, n_estimators=100..............................\n",
      "[CV 3/3; 9/9] END ............max_depth=15, n_estimators=100; total time=  28.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=15, random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators':[50,70,100],'max_depth':[5,10,15]}\n",
    "forest = RandomForestClassifier(random_state=42,class_weight='balanced')\n",
    "forest_clf = GridSearchCV(forest, parameters,cv=3,scoring='f1',verbose=10)\n",
    "forest_clf.fit(tf_idf_train, y_train)\n",
    "forest_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "332da4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3667450502786933"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b0d1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dict['RandomForest'] = [forest_clf.best_score_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5592f2",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5361582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(tf_idf_train, y_train)\n",
    "test_pool = Pool(tf_idf_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ba79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(logging_level='Silent',eval_metric='TotalF1',class_weights=[0.1,0.9],learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49bc2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'depth': [10],\n",
    "        'iterations':[70, 100]\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57bf3c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c986f062a9486297f58a6bd482e793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_result = model.grid_search(grid,\n",
    "            train_pool,\n",
    "            cv=3,\n",
    "            verbose=False,\n",
    "            plot=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2be83f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8665582458368265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(grid_search_result['cv_results']['test-TotalF1-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "563d930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dict['CatBoost'] = [max(grid_search_result['cv_results']['test-TotalF1-mean'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75f17cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 10, 'iterations': 100}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_result['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df09a1ae",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81d10442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      F1\n",
       "LogisticRegression  0.87\n",
       "CatBoost            0.87\n",
       "RandomForest        0.37"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(valid_dict,orient='index',columns=['F1']).round(decimals=2).sort_values(by='F1',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cb5b7",
   "metadata": {},
   "source": [
    "Наилучший результат показала логистическая регрессия. Проверим её на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e20f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = reg.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af9d2f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567567567567"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7de937",
   "metadata": {},
   "source": [
    "Логистическая регрессия прошла проверку."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87df16",
   "metadata": {},
   "source": [
    "Проверим модель на адекватность. Пусть наша модель предсказывает, что все отзывы негативные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edaf9ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18445827349609065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score([1]*len(y_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14d4d7",
   "metadata": {},
   "source": [
    "А если все отзывы положительные?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46475fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score([0]*len(y_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac1319",
   "metadata": {},
   "source": [
    "Наша модель определенно работает лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1f66f",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "- Логистическая регрессия показала наилучший результат и прошла пороговое значение в 0.75 на тестовой выборке. И обучается она занчительно быстрее, чем остальные методы. Оставляем логистическую регрессию.\n",
    "- Если брать все отзывы негативными, то занчение F1 получается значительно меньше, чем в логистической регрессии, значит наша модель умеет достаточно хорошо предсказывать тип отзыва. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
